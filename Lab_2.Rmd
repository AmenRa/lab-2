---
title: "Lab 2 - Classification"
output: html_notebook
---
# Packages
## Install and import packages
```{r}
# List of packages needed for this tutorial
list.of.packages <- c(
    "kernlab",
    "caret",
    "tm",
    "dplyr",
    "splitstackshape",
    "e1071",
    "textclean",
    "mgsub",
    "tictoc",
    "klaR",
    "promises",
    "C50",
    "inum"
)

# Check which packages have not been installed yet
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]

# If there are some packages that have not been installed yet, install them
if (length(new.packages)) {
    install.packages(new.packages)
}
```

## Import Packages
```{r}
# Import packages
library("dplyr")
library(data.table)
library("lattice")
library("ggplot2")
library("textclean")
library("mgsub")
library("stringi")
library("kernlab")
library("caret")
library("tm")
library("dplyr")
library("splitstackshape")
library("e1071")
library("tictoc")
```

# Dataset
## Import Dataset
In this tutorial we use the **Reuters dataset**, which is a well-known dataset composed of several texts and used for **text classification** research.

Reuters is an international business and financial news agency.

The news in the dataset are labelled according to their topics.

The most populated topics are:

1. **acq:** Mergers/Acquisitions;
2. **earn:** Earnings and Earnings Forecasts;
3. **money.fx:** Money/Foreign Exchange.

```{r}
# Import dataset from csv
original_dataset <- fread("reuters_dataset.csv", encoding = 'UTF-8')

# Get the number of rows in dataset
nrow(original_dataset)
```

```{r}
# Print length of the most populated topics (classes) in the dataset
# Since articles can be classified in more than one class we print disjointet sets
cat('topic.acq', length(which(original_dataset$topic.acq == 1
                              & original_dataset$topic.earn == 0
                              & original_dataset$topic.money.fx == 0)), '\n')

cat('topic.earn', length(which(original_dataset$topic.acq == 0
                               & original_dataset$topic.earn == 1
                               & original_dataset$topic.money.fx == 0)), '\n')

cat('topic.money.fx', length(which(original_dataset$topic.acq == 0
                                   & original_dataset$topic.earn == 0
                                   & original_dataset$topic.money.fx == 1)), '\n')
```

As you can see the three classes have different cardinalities.

## Create a Balanced Dataset

It is a good practice in Machine Learning to use only _balanced_ datasets.
A **balanced dataset** is a dataset composed of an equal number of samples per class.

For this reason we want to compose a new balanced dataset, conatining only articles regarding the three most popular topics in the original dataset.

```{r}
# Create a new original_dataset containing only articles related to acq, earn and money.fx
# Each article must belong to a single class
three_class_dataset = original_dataset[
  (original_dataset$topic.acq == 1 & original_dataset$topic.earn == 0 & original_dataset$topic.money.fx == 0)
  | (original_dataset$topic.acq == 0 & original_dataset$topic.earn == 1 & original_dataset$topic.money.fx == 0)
  | (original_dataset$topic.acq == 0 & original_dataset$topic.earn == 0 & original_dataset$topic.money.fx == 1)]
```

```{r}
# Take 800 articles from acq
acq_articles = three_class_dataset[three_class_dataset$topic.acq == 1]
acq_articles = acq_articles[1:800,]
```

```{r}
# Take 800 articles from earn
earn_articles = three_class_dataset[three_class_dataset$topic.earn == 1]
earn_articles = earn_articles[1:800,]
```

```{r}
# Take 800 articles from money
money_articles = three_class_dataset[three_class_dataset$topic.money == 1]
money_articles = money_articles[1:800,]
```

```{r}
# Put the parts together and create the dataset
dataset <- rbind(acq_articles, earn_articles, money_articles)
```

```{r}
# Select columns "pid" (article ids) and "doc.text" (article texts)
dataset = select(dataset, pid, doc.text)
```

```{r}
# Rename columns "pid"-> "doc_id", "doc.text" -> "text"
colnames(dataset)[1] <- "doc_id"
colnames(dataset)[2] <- "text"
```

```{r}
# Print dataset number of articles
cat('dataset articles: ', nrow(dataset))
```

# Data Preprocessing

Lets define a function for data preprocessing that can be used when needed.

```{r}
# Data Preprocessing
preprocess_dataset <- function(set) {
  # Tranform the input in a VCorpus (datastructure provided by tm)
  corpus <- VCorpus(DataframeSource(set))
  # Strip white spaces at the beginning and at the end of preprocessing
  # in order to avoid some problems later
  corpus <- tm_map(corpus, content_transformer(stripWhitespace))
  # User replace_contraction function from textclean package
  corpus <- tm_map(corpus, content_transformer(replace_contraction))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, content_transformer(removePunctuation))
  corpus <- tm_map(corpus, content_transformer(removeNumbers))
  corpus <- tm_map(corpus, stemDocument, language = "english")
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  corpus <- tm_map(corpus, content_transformer(stripWhitespace))

  return(corpus)
}
```

Now that the preprocessing function as been defined we can call it on the training and test set in order to obtain clean sets.

```{r}
# Preprocess dataset
print("Starting dataset preprocessing...")
dataset <- preprocess_dataset(dataset)
print("Dataset preprocessing: done")
```

## Create Representations

Now it's time to create the actual representations of the texts contained in the dataset. They will be used to feed the Machine Learning algorithms.

### Feature Selection

A good practice before feeding samples to a Machine Learning algorithm is to preprocess samples removing low-relevant features from the representations, this step is called **Feature Selection**.

Feature selection techniques are used for four reasons:

1. Simplification of models to make them easier to interpret by researchers/users (the problem of _"explainability"_ is critic in field such as finance, heatlh, etc.);
2. Shorter training times (Simplier samples = faster learning);
3. To avoid the _curse of dimensionality_ (An enormous amount of training data is required to ensure that there are several samples with each combination of values if the feature dimensionality is too high);
4. Enhanced generalization by reducing _overfitting_ ("the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably", Oxford Dictionaries - It happens when the algorithm is "learning the samples" in the training set and not a real function);

It is easy to understand that in a text classification task rare (very low frequency) terms bring very little information as they are not useful for generalization but only in the representation of very specific samples and we do not want our model to learn samples (overfitting!).

In the same way, very high frequency terms are useless, in fact we already removed stopwords.

Then we define a feature selection function that can be called every time it's needed:

```{r}
# Feature selection
apply_feature_selection_on_dtm <- function(dtm_fs, sparsity_value = 0.99, verbose = FALSE) {
  if (verbose) {
    print("DTM before sparse term removal")
    inspect(dtm_fs)
  }

  dtm_fs = removeSparseTerms(dtm_fs, sparsity_value)

  if (verbose) {
    print("DTM after sparse term removal")
    inspect(dtm_fs)
  }

  return(dtm_fs)
}
```

### Corpus to Matrix Functions

```{r}
# Binary matrix
create_binary_matrix <- function(corpus, sparsity_value, verbose) {
  if (verbose) {
    print("Creating binary matrix...")
  }
    
  dtm_binary <- DocumentTermMatrix(corpus, control = list(weighting = weightBin))
  dtm_binary <- apply_feature_selection_on_dtm(dtm_binary, sparsity_value, verbose)
  matrix_binary <- as.matrix(dtm_binary)
  return(matrix_binary)
}
```

```{r}
# Bigram binary matrix
create_bigram_binary_matrix <- function(corpus, sparsity_value, verbose) {
  if (verbose) {
    print("Creating bigram binary matrix...")
  }
    
  BigramTokenizer <- function(x) {
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
  }
    
  dtm_bigram_binary <- DocumentTermMatrix(corpus, control = list(tokenize = BigramTokenizer, weighting = weightBin))
  dtm_bigram_binary <- apply_feature_selection_on_dtm(dtm_bigram_binary, sparsity_value, verbose)
  matrix_bigram_binary <- as.matrix(dtm_bigram_binary)
  return(matrix_bigram_binary)
}
```

<h4 style="color:red">EXERCISE: complete the other "corpus to matrix" functions</h4>

```{r}
# TF matrix
create_tf_matrix <- function(corpus, sparsity_value, verbose) {
  if (verbose) {
    print("Creating tf matrix...")
  }

  ...
}
```

```{r}
# Bigram TF matrix
create_bigram_tf_matrix <- function(corpus, sparsity_value, verbose) {
  if (verbose) {
    print("Creating bigram tf matrix...")
  }

  ...
}
```

```{r}
# TF-IDF matrix
create_tfidf_matrix <- function(corpus, sparsity_value, verbose) {
  if (verbose) {
    print("Creating tf-idf matrix...")
  }

  ...
}
```

```{r}
# Bigram TF-IDF matrix
create_bigram_tfidf_matrix <- function(corpus, sparsity_value, verbose) {
  if (verbose) {
    print("Creating bigram tf-idf matrix...")
  }

  ...
}
```

### Create Matrix Function

On the top of the previous functions we build another function that permit us to call them and to control their parameters.

```{r}
# Create matrix
create_matrix <- function(corpus, matrix_type, sparsity_value = 0.99, verbose = NULL) {
  if (matrix_type == 'binary') {
    matrix <- create_binary_matrix(corpus, sparsity_value, verbose)
  } else if (matrix_type == 'bigram_binary') {
    matrix <- create_bigram_binary_matrix(corpus, sparsity_value, verbose)
  } else if (matrix_type == 'tf') {
    matrix <- create_tf_matrix(corpus, sparsity_value, verbose)
  } else if (matrix_type == 'bigram_tf') {
    matrix <- create_bigram_tf_matrix(corpus, sparsity_value, verbose)
  } else if (matrix_type == 'tfidf') {
    matrix <- create_tfidf_matrix(corpus, sparsity_value, verbose)
  } else if (matrix_type == 'bigram_tfidf') {
    matrix <- create_bigram_tfidf_matrix(corpus, sparsity_value, verbose)
  } else {
    print('Invalid matrix type!')
  }
  return(matrix)
}
```

### Create representations

Now we can create all the different matrices using the functions defined above.

We will do it later, I leave here some commented lines of code for testing/debugging purpose.

```{r}
# dataset_matrix <- create_matrix(dataset, 'binary', sparsity_value=0.95, verbose=TRUE)
```

#### Find Representations' Intersection

It is necessary to represent both sets (training and test) in the same way in order to use them in a Machine Learning task, so we also need to represent them with the intersection of the terms used to represent each one:

```{r}
find_intersection_and_create_dataframe <- function(matrix_1, matrix_2) {
  intersection_matrix <- data.frame(matrix_1[,intersect(colnames(matrix_1), colnames(matrix_2))])
  return(intersection_matrix)
}
```

### Label Training and Test Data

Label each sample with the topic they are classified in.

```{r}
label_dataset <- function(df) {
  df$Topic <- ''
  df[1:800,]$Topic <- 'acq'
  df[801:1600,]$Topic <- 'earn'
  df[1601:2400,]$Topic <- 'money_fx'
  return(df)
}
```

### Dataset Splitting

Another good practice is to divide the dataset into two subsets: **training set** and **test set**.

1. **Training set:** The sample of data used to _fit_ the model. The actual dataset that we use to _train_ the model. The model sees and learns from this data. (Usually 75/80% of the dataset)


2. **Test set:** The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. The Test dataset provides the _gold standard_ used to evaluate the model. It is only used once a model is completely trained. (The rest of the dataset - 20/25%)

```{r}
split_dataset <- function(x) {
    # Take 600 acq articles for training set
    train_acq = x[1:600,]
    # Take 200 acq articles for test set
    test_acq = x[601:800,]

    # Take 600 earn articles for training set
    train_earn = x[801:1400,]
    # Take 200 earn articles for test set
    test_earn = x[1401:1600,]

    # Take 600 money articles for training set
    train_money_fx = x[1601:2200,]
    # Take 200 money articles for test set
    test_money_fx = x[2201:2400,]

    # Put parts togheter and create training set
    train_set <- rbind(train_acq, train_earn, train_money_fx)

    # Put parts togheter and create test set
    test_set <- rbind(test_acq, test_earn, test_money_fx)
    
    return(list(train_set, test_set))
}
```

### Summarize Dataset Distribution

We can summarize the class/topic distribution of training and test set with the following function:

```{r}
summarize_distribution <- function(df) {
  df_percentage <- prop.table(table(df$Topic)) * 100
  distribution_summary <- cbind(freq=table(df$Topic), df_percentage)
  return(distribution_summary)
}
```

# Text Classification

## Metrics & Evaluation Protocol

Before proceeding with the experiment we have to choose a _"measure of success"_ (the metric(s) to use to evaluate the experiments) and to decide on an _"evaluation protocol"_.

There are several different metrics used to evaluate Machine Learning models (we refer to Machine Learning models as the output of an algorithm's training process), in this tutorial, for simplicity, we use only **accuracy**.

<br />
$$
  accuracy = \frac{number\ of\ correct\ predictions}{total\ number\ of\ predictions}
$$
<br />

As evaluation protocol we use the "**K-fold Cross Validation**". It can be summarized in the following steps:

1. split data into $K$ partitions of equal size;
2. for each partition $i$, train a model on the remaining $K – 1$ partitions, and evaluate it on partition $i$;
3. the final score is the average of the $K$ scores obtained.

<center>
    ![](./images/k-fold.png)
</center>

As said above, during the validation process, the training set is splitted in 2 parts, one used for training and one for evaluation. The second one is called **validation set**.

**Validation set:** The sample of data used to provide an unbiased evaluation of a model fit on the training set while tuning model hyperparameters (model initialization parameters). The evaluation becomes more biased as information on the validation dataset is incorporated into the model configuration.

```{r}
# Define two variables used to tell ML algorithm how to run and evaluate experiments
# cv = cross validation, number = K
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
```

## Model Training Functions

We are now ready do define some functions that train Machine Learning models.

**HINT**: Tic/Toc functions are useful to monitor elapsed time.

```{r}
# Decision Tree C5.0
train_dt_classifier <- function(train_df, metric, control) {
  library("C50")
  # Start timer...
  tic("Decision Tree")
  set.seed(7)
  model <- train(Topic~., data=train_df, method="C5.0", metric=metric, trControl=control)
  # Stop timer...
  toc()
  return(model)
  detach("package:C50", unload=TRUE)
}
```

```{r}
# Support Vector Machine
train_svm_classifier <- function(train_df, metric, control) {
  tic("SVM")
  set.seed(7)
  model <- train(Topic~., data=train_df, method="svmRadial", metric=metric, trControl=control)
  toc()
  return(model)
}
```

```{r}
# K-Nearest Neighbors
train_knn_classifier <- function(train_df, metric, control) {
  tic("KNN")
  set.seed(7)
  model <- train(Topic~., data=train_df, method="knn", metric=metric, trControl=control)
  toc()
  return(model)
}
```

```{r}
# Random Forest
train_rf_classifier <- function(train_df, metric, control) {
  tic("Random Forest")
  set.seed(7)
  model <- train(Topic~., data=train_df, method="rf", metric=metric, trControl=control)
  toc()
  return(model)
}
```

```{r}
# Neural Networks
train_nn_classifier <- function(train_df, metric, control) {
  tic("Neural Networks")
  set.seed(7)
  model <- train(Topic~., data=train_df, method="nnet", metric=metric, trControl=control, trace=FALSE)
  toc()
  return(model)
}
```

# Experiments

Now we are ready to put all this code together and get the results!

```{r}
# Possible values:  binary, bigram_binary, tf, bigram_tf, tfidf, bigram_tfidf
wanted_matrix_type <- 'binary'
wanted_sparsity_value <- 0.95
wanted_verbose <- FALSE

dataset_matrix <- create_matrix(
    dataset,
    wanted_matrix_type,
    wanted_sparsity_value,
    wanted_verbose
)

# Convert matrix into datafram
dataset_df <- data.frame(dataset_matrix)

# Label dataset
labeled_dataset_df <- label_dataset(dataset_df)

# Split dataset in training and test sets
splitted_dataset = split_dataset(labeled_dataset_df)
train_df = splitted_dataset[[1]]
test_df = splitted_dataset[[2]]

# Summarize distributions
print(summarize_distribution(train_df))
print(summarize_distribution(test_df))
```

Once the representations have been created, it is possible to train the classifiers and to compare their training performances.
(The training step could requrie several minutes!)

```{r}
  # Training
  control <- trainControl(method="cv", number=5)
  metric <- "Accuracy"

  dt_model <- train_dt_classifier(train_df, metric, control)
  svm_model <- train_svm_classifier(train_df, metric, control)
  knn_model <- train_knn_classifier(train_df, metric, control)
  rf_model <- train_rf_classifier(train_df, metric, control)
  nn_model <- train_nn_classifier(train_df, metric, control)
```

```{r}
  # Compare training performances
  results <- resamples(list(
    dt=dt_model,
    svm=svm_model,
    knn=knn_model,
    rf=rf_model,
    nn=nn_model
  ))
  summary(results)

  cat('Decision Tree training accuracy: ', summary(results)$statistics$Accuracy[1, 4], '\n')
  cat('SVM training accuracy: ', summary(results)$statistics$Accuracy[2, 4], '\n')
  cat('KNN training accuracy: ', summary(results)$statistics$Accuracy[3, 4], '\n')
  cat('Random Forest training accuracy: ', summary(results)$statistics$Accuracy[4, 4], '\n')
  cat('Neural Network training accuracy: ', summary(results)$statistics$Accuracy[5, 4], '\n')
```

Now let's test our models and compare their performances on the test set.
```{r}
  dt_predictions <- predict(dt_model, newdata = test_df)
  dt_confusion_matrix <- confusionMatrix(table(dt_predictions, test_df$Topic))
  cat('Decision Tree test accuracy: ', unname(dt_confusion_matrix$overall[1]), '\n')

  svm_predictions <- predict(svm_model, newdata = test_df)
  svm_confusion_matrix <- confusionMatrix(table(svm_predictions, test_df$Topic))
  cat('SVM test accuracy: ', unname(svm_confusion_matrix$overall[1]), '\n')

  knn_predictions <- predict(knn_model, newdata = test_df)
  knn_confusion_matrix <- confusionMatrix(table(knn_predictions, test_df$Topic))
  cat('KNN test accuracy: ', unname(knn_confusion_matrix$overall[1]), '\n')

  rf_predictions <- predict(rf_model, newdata = test_df)
  rf_confusion_matrix <- confusionMatrix(table(rf_predictions, test_df$Topic))
  cat('Random Forest test accuracy: ', unname(rf_confusion_matrix$overall[1]), '\n')

  nn_predictions <- predict(nn_model, newdata = test_df)
  nn_confusion_matrix <- confusionMatrix(table(nn_predictions, test_df$Topic))
  cat('Neural Networks test accuracy: ', unname(nn_confusion_matrix$overall[1]), '\n')
```

<h4 style="color:red">EXERCISE: run all the algorithms for all the different representations, keep track of the results in the file named lab_2_results.xlsx (or in any other file/software you are familiar with - even pen&paper)</h4>

<br />
<br />
<br />
<br />
<br />
<br />
